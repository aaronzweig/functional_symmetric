{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from model import Symmetric, DeepSets, KNN, KK\n",
    "from sample import generate_data, generate_narrow_data\n",
    "from train import train\n",
    "from evaluate import generalization_error, cross_validate\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For smooth neuron experiment, it's only fair to S2 if the neuron is drawn from the same random features\n",
    "\n",
    "def smooth_neuron_weight_init(model, objective):\n",
    "    if objective.__name__ == \"smooth_neuron\":\n",
    "        if model.__name__ == \"S2\" or model.__name__ == \"S3\":\n",
    "            with torch.no_grad():\n",
    "                m = objective.__network__.phi.fc.weight.shape[0]\n",
    "                model.phi.fc.weight[:m] = objective.__network__.phi.fc.weight\n",
    "                model.phi.fc.weight.div_(torch.norm(model.phi.fc.weight, dim = 1, keepdim = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, objective, narrow, \n",
    "                   verbose = True, log_plot = False, scaleup = False, kernel_buff = False, squared = False):\n",
    "    \n",
    "    print(\"currently\", objective.__name__)\n",
    "    \n",
    "    bias_first = \"neuron\" in objective.__name__\n",
    "\n",
    "    c = 1 if not scaleup else 2\n",
    "    \n",
    "    k = 10 if kernel_buff else 1\n",
    "\n",
    "    f1 = Symmetric(input_dim, c * hidden_dim, hidden_dim, squared = squared)\n",
    "    f2 = KNN(input_dim, c * k * hidden_dim, hidden_dim, squared = squared)\n",
    "    f3 = KK(input_dim, c * k * hidden_dim, k * hidden_dim, squared = squared)\n",
    "\n",
    "    f1.__name__ = \"S1\"\n",
    "    f2.__name__ = \"S2\"\n",
    "    f3.__name__ = \"S3\"\n",
    "\n",
    "    models = [f1, f2, f3]\n",
    "    lambs = [0., 1e-6, 1e-4, 1e-2]\n",
    "    N_list = np.arange(2, N_max + 16)\n",
    "\n",
    "    for model in models:\n",
    "        x, y = generate_data(N_max, batch_size, input_dim, objective, narrow, bias_first)\n",
    "        \n",
    "        smooth_neuron_weight_init(model, objective)\n",
    "        \n",
    "        cv_models = cross_validate(model, x, y, iterations, lambs, verbose)\n",
    "        \n",
    "        validation_errors = np.zeros_like(lambs)\n",
    "        for i, cv_model in enumerate(cv_models):\n",
    "            validation_errors[i] = generalization_error([N_max], 1000, input_dim, cv_model,\n",
    "                                                        objective, narrow, bias_first)[0]\n",
    "        \n",
    "        i = np.argmin(validation_errors)\n",
    "        lamb = lambs[i]\n",
    "            \n",
    "        runs = 10\n",
    "        run_errors = np.zeros((runs, len(N_list)))\n",
    "        for i in range(runs):\n",
    "            x, y = generate_data(N_max, batch_size, input_dim, objective, narrow, bias_first)\n",
    "            model_copy = copy.deepcopy(model)\n",
    "            model_copy.reinit()\n",
    "            smooth_neuron_weight_init(model_copy, objective)\n",
    "            \n",
    "            train(model_copy, x, y, iterations, lamb)\n",
    "            errors = generalization_error(N_list, 1000, input_dim, model_copy, objective, narrow, bias_first)\n",
    "            run_errors[i] = np.array(errors)\n",
    "        \n",
    "        mean_error = np.mean(run_errors, axis = 0)\n",
    "        std_error = np.std(run_errors, axis = 0)\n",
    "        if verbose:\n",
    "            print(\"performance of \", model.__name__, \" on \", objective.__name__)\n",
    "            print(\"lamb =\", lamb)\n",
    "            print(mean_error)\n",
    "            print(std_error)\n",
    "            \n",
    "            \n",
    "        narrow_str = \"Narrow\" if narrow else \"Wide\"\n",
    "        scaleup_str = \"scaleup\" if scaleup else \"\"\n",
    "        save_str = model.__name__ + \"_\" + objective.__name__ + \"_\" + narrow_str + \"_\" + str(input_dim)\n",
    "        save_str += \"_\" + str(scaleup) + \"_\" + str(kernel_buff) + \"_\" + str(squared)\n",
    "        save_dir = \"saved_data_2022/\"\n",
    "            \n",
    "        np.save(save_dir + save_str + \"_mean\", mean_error)\n",
    "        np.save(save_dir + save_str + \"_std\", std_error)\n",
    "        \n",
    "        if log_plot:\n",
    "            plt.semilogy(N_list, mean_error, label = model.__name__)\n",
    "        else:\n",
    "            plt.plot(N_list, mean_error, label = model.__name__)\n",
    "        plt.fill_between(N_list, mean_error - std_error, mean_error + std_error, alpha = 0.2)\n",
    "\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.ylim([1e-5, 1e2]) \n",
    "\n",
    "    plt.xlabel(\"N\")\n",
    "    plt.ylabel(\"Mean Square Error\")\n",
    "    narrow_str = \"Narrow\" if narrow else \"Wide\"\n",
    "    plt.title(narrow_str + \" generalization for \" + objective.__name__)\n",
    "    scaleup_str = \"scaleup\" if scaleup else \"\"\n",
    "#     plt.savefig(\"plots_high_dim/\" + objective.__name__ + \"_\" + narrow_str + \"_\" + str(input_dim) + scaleup_str)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the rest of the notebook\n",
    "input_dim = 10\n",
    "hidden_dim = 100\n",
    "squared = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = lambda x: np.mean(norm(x, axis = 2), axis = 1, keepdims = True)\n",
    "\n",
    "median = lambda x: np.median(norm(x, axis = 2), axis = 1, keepdims = True)\n",
    "\n",
    "maximum = lambda x: np.max(norm(x, axis = 2), axis = 1, keepdims = True)\n",
    "\n",
    "lamb = 0.1\n",
    "softmax = lambda x: lamb * np.log(np.mean(np.exp(norm(x, axis = 2) / lamb), axis = 1, keepdims = True))\n",
    "\n",
    "second = lambda x: np.sort(norm(x, axis = 2), axis = 1)[:,-2].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_flip = lambda x: np.mean(1.0 / norm(x, axis = 2), axis = 1, keepdims = True)\n",
    "\n",
    "median_flip = lambda x: np.median(1.0 / norm(x, axis = 2), axis = 1, keepdims = True)\n",
    "\n",
    "maximum_flip = lambda x: np.max(1.0 / norm(x, axis = 2), axis = 1, keepdims = True)\n",
    "\n",
    "lamb = 0.1\n",
    "softmax_flip = lambda x: lamb * np.log(np.mean(np.exp(1.0 / norm(x, axis = 2) / lamb), axis = 1, keepdims = True))\n",
    "\n",
    "second_flip = lambda x: np.sort(1.0 / norm(x, axis = 2), axis = 1)[:,-2].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "def potential(x):\n",
    "    energies = np.zeros((x.shape[0], 1))\n",
    "    for i in range(x.shape[0]):\n",
    "\n",
    "        r = x[i]\n",
    "        D = distance_matrix(r, r)\n",
    "\n",
    "        np.fill_diagonal(D, 1)\n",
    "        D = 1.0/D\n",
    "        \n",
    "        m = D.shape[0]\n",
    "        r,c = np.triu_indices(m,1)\n",
    "        D = D[r,c]\n",
    "        energies[i] = -np.mean(D)\n",
    "        \n",
    "    return energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixture(tensor, mean_1, std_1, mean_2, std_2):\n",
    "    with torch.no_grad():\n",
    "        x_1 = mean_1 + std_1 * torch.randn_like(tensor)\n",
    "        x_2 = mean_2 + std_2 * torch.randn_like(tensor)\n",
    "        \n",
    "        p = torch.bernoulli(torch.zeros_like(tensor) + 0.5)\n",
    "        tensor.data = p * x_1 + (1-p) * x_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.9413853  -0.9735007   0.         -0.6611508  -0.17372644 -0.10888965\n",
      " -3.002563   -0.89388555 -1.2175204  -3.8266776  -0.3530697  -2.9725735\n",
      " -0.36615312 -1.504894   -0.9650178 ]\n"
     ]
    }
   ],
   "source": [
    "### May need to sample several neurons to find one that isn't degenerate on the domain\n",
    "\n",
    "torch.manual_seed(50)\n",
    "np.random.seed(50)\n",
    "\n",
    "for i in range(100):\n",
    "    teacher = Symmetric(input_dim, 1, 1, squared = squared)\n",
    "    mixture(teacher.phi.fc.weight, 1.0, 0.5, -1.0, 0.5)\n",
    "    teacher.eval()\n",
    "\n",
    "    def neuron(x):\n",
    "        x = torch.from_numpy(x).float()\n",
    "        y = teacher(x)\n",
    "        return y.data.numpy().reshape(-1, 1)\n",
    "\n",
    "    neuron.__network__ = teacher\n",
    "\n",
    "    x, y = generate_narrow_data(3, 15, input_dim, neuron, bias_first = True)\n",
    "    z = y.data.numpy().flatten()\n",
    "    print(z)\n",
    "    if np.count_nonzero(z==0) >= 1 and np.count_nonzero(z==0) < 4 and np.max(np.abs(z)) >= 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02960487  0.          0.         -0.02094294 -0.03513904 -0.02753521\n",
      " -0.02093345 -0.00561744  0.         -0.03878238 -0.00878886 -0.00971054\n",
      " -0.00958526 -0.0464745  -0.00434159]\n"
     ]
    }
   ],
   "source": [
    "### May need to sample several neurons to find one that isn't degenerate on the domain\n",
    "\n",
    "smooth_teacher = Symmetric(input_dim, hidden_dim, 1, squared = squared)\n",
    "# mixture(smooth_teacher.rho.fc1.weight, 0.5, 0.5, -0.5, 0.5)\n",
    "smooth_teacher.eval()\n",
    "def smooth_neuron(x):\n",
    "        x = torch.from_numpy(x).float()\n",
    "        y = smooth_teacher(x)\n",
    "        return y.data.numpy().reshape(-1, 1)\n",
    "\n",
    "smooth_neuron.__network__ = smooth_teacher\n",
    "\n",
    "x, y = generate_narrow_data(3, 15, input_dim, smooth_neuron, bias_first = True)\n",
    "print(y.data.numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron.__name__ = \"neuron\"\n",
    "smooth_neuron.__name__ = \"smooth_neuron\"\n",
    "maximum.__name__ = \"maximum\"\n",
    "softmax.__name__ = \"softmax\"\n",
    "median.__name__ = \"median\"\n",
    "mean.__name__ = \"mean\"\n",
    "second.__name__ = \"second\"\n",
    "potential.__name__ = \"potential\"\n",
    "\n",
    "\n",
    "maximum_flip.__name__ = \"maximum_flip\"\n",
    "softmax_flip.__name__ = \"softmax_flip\"\n",
    "median_flip.__name__ = \"median_flip\"\n",
    "mean_flip.__name__ = \"mean_flip\"\n",
    "second_flip.__name__ = \"second_flip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run to generate plots in Figure 1:\n",
    "\n",
    "N_max = 4\n",
    "\n",
    "iterations = 5000\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, smooth_neuron, narrow = False, log_plot = True,\n",
    "#               kernel_buff = True, squared = squared)\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, neuron, narrow = False, log_plot = True,\n",
    "#               kernel_buff = True, squared = squared)\n",
    "\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, smooth_neuron, narrow = False, log_plot = True,\n",
    "#               kernel_buff = True, squared = squared, scaleup = True)\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, neuron, narrow = False, log_plot = True,\n",
    "#               kernel_buff = True, squared = squared, scaleup = True)\n",
    "\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, smooth_neuron, narrow = True, log_plot = True,\n",
    "#               kernel_buff = True, squared = squared)\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, neuron, narrow = True, log_plot = True,\n",
    "#               kernel_buff = True, squared = squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently potential\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ed7ca16f55f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, potential, narrow = False, log_plot = True,\n\u001b[0;32m----> 2\u001b[0;31m               kernel_buff = True, squared = squared)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, potential, narrow = True, log_plot = True,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#               kernel_buff = True, squared = squared)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-ce337c705a7d>\u001b[0m in \u001b[0;36mcompare_models\u001b[0;34m(N_max, hidden_dim, iterations, batch_size, input_dim, objective, narrow, verbose, log_plot, scaleup, kernel_buff, squared)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0msmooth_neuron_weight_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mcv_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mvalidation_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/symmetric_mean_field/evaluate.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(model, x, y, iterations, lambs, verbose)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlamb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlambs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mmodel_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_copy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlamb\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/symmetric_mean_field/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, x, y, iterations, lamb, lr)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlamb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/symmetric_mean_field/model.py\u001b[0m in \u001b[0;36mregularize\u001b[0;34m(self, lamb)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mreg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mreg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlamb\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreg_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, potential, narrow = False, log_plot = True,\n",
    "              kernel_buff = True, squared = squared)\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, potential, narrow = True, log_plot = True,\n",
    "#               kernel_buff = True, squared = squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, mean_flip, narrow = False, log_plot = True,\n",
    "              kernel_buff = True, squared = squared)\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim , mean, narrow = True, log_plot = True)\n",
    "\n",
    "compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, median_flip, narrow = False, log_plot = True,\n",
    "              kernel_buff = True, squared = squared)\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim , median, narrow = True, log_plot = True)\n",
    "\n",
    "compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, maximum_flip, narrow = False, log_plot = True,\n",
    "              kernel_buff = True, squared = squared)\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim , maximum, narrow = True, log_plot = True)\n",
    "\n",
    "compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, softmax_flip, narrow = False, log_plot = True,\n",
    "              kernel_buff = True, squared = squared)\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim , softmax, narrow = True, log_plot = True)\n",
    "\n",
    "compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, second_flip, narrow = False, log_plot = True,\n",
    "              kernel_buff = True, squared = squared)\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim , second, narrow = True, log_plot = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = smooth_neuron\n",
    "narrow = False\n",
    "bias_first = \"neuron\" in objective.__name__\n",
    "\n",
    "kernel_buff = True\n",
    "k = 10 if kernel_buff else 1\n",
    "\n",
    "x, y = generate_data(N_max, batch_size, input_dim, objective, narrow, bias_first)\n",
    "\n",
    "for i in range(5):\n",
    "                \n",
    "    model = Symmetric(input_dim, hidden_dim, hidden_dim, squared = squared)\n",
    "    model.train()\n",
    "    losses = train(model, x, y, iterations, lamb = 0.00, lr=0.0005)\n",
    "    model.eval()\n",
    "    print(losses[::int(iterations/10)])\n",
    "    print(\"min\", np.min(np.array(losses)))\n",
    "    print(\"f1\", generalization_error([4], 5000, input_dim, model, objective, narrow, bias_first))\n",
    "                \n",
    "    model = KNN(input_dim, k * hidden_dim, hidden_dim, squared = squared)\n",
    "    model.__name__ = \"S2\"\n",
    "    smooth_neuron_weight_init(model, objective)\n",
    "    model.train()\n",
    "    losses = train(model, x, y, iterations, lamb = 0.00, lr=0.0005)\n",
    "    model.eval()\n",
    "    print(losses[::int(iterations/10)])\n",
    "    print(\"min\", np.min(np.array(losses)))\n",
    "    print(\"f2\", generalization_error([4], 5000, input_dim, model, objective, narrow, bias_first))\n",
    "    \n",
    "    model = KK(input_dim, k * hidden_dim, k * hidden_dim, squared = squared)\n",
    "    model.__name__ = \"S3\"\n",
    "    smooth_neuron_weight_init(model, objective)\n",
    "    model.train()\n",
    "    losses = train(model, x, y, iterations, lamb = 0.00, lr=0.0005)\n",
    "    model.eval()\n",
    "    print(losses[::int(iterations/10)])\n",
    "    print(\"min\", np.min(np.array(losses)))\n",
    "    print(\"f3\", generalization_error([4], 5000, input_dim, model, objective, narrow, bias_first))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_from_memory(yrange, input_dim, objectives, narrows, scaleup,\n",
    "                     kernel_buff = True, squared = True, logplot = True):\n",
    "    models = [\"S1\", \"S2\", \"S3\"]\n",
    "    N_list = np.arange(2, 4 + 16)\n",
    "    \n",
    "    for objective in objectives:\n",
    "        for narrow in narrows:\n",
    "            \n",
    "            save_str = objective + \"_\" + narrow + \"_\" + str(input_dim)\n",
    "            save_str += \"_\" + str(scaleup) + \"_\" + str(kernel_buff) + \"_\" + str(squared)\n",
    "            \n",
    "            for model in models:\n",
    "        \n",
    "                save_dir = \"saved_data_2022/\"\n",
    "        \n",
    "                mean_error = np.load(save_dir + model + \"_\" +  save_str + \"_mean\" + \".npy\")\n",
    "                std_error = np.load(save_dir + model + \"_\" + save_str + \"_std\" + \".npy\")\n",
    "                \n",
    "                if logplot:\n",
    "                    plt.semilogy(N_list, mean_error, label = model)\n",
    "                else:\n",
    "                    plt.plot(N_list, mean_error, label = model)\n",
    "                plt.fill_between(N_list, mean_error - std_error, mean_error + std_error, alpha = 0.2)\n",
    "\n",
    "\n",
    "            plt.legend()\n",
    "            plt.ylim(yrange) \n",
    "\n",
    "            plt.xlabel(\"N\")\n",
    "            plt.ylabel(\"Mean Square Error\")\n",
    "            narrow_str = narrow\n",
    "            \n",
    "            objective_str = objective.replace(\"_flip\", \"\")\n",
    "            \n",
    "#             plt.title(narrow_str + \" generalization for \" + objective_str)\n",
    "            plt.title(\"Test Error for \" + objective_str)\n",
    "            plt.savefig(\"plots_high_dim_2022/\" + save_str)\n",
    "            plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_input_dim = 10\n",
    "\n",
    "# plot_from_memory([1e-3 * 0.5, 1e-1 * 2], local_input_dim, \n",
    "#                  [\"mean_flip\", \"median_flip\", \"maximum_flip\", \"softmax_flip\", \"second_flip\", \"potential\"],\n",
    "#                  [\"Wide\"], scaleup = False, kernel_buff = True, squared = True)\n",
    "\n",
    "# plot_from_memory([1e-1, 1e1*2], local_input_dim, [\"neuron\"], [\"Wide\"],\n",
    "#                  scaleup = False, kernel_buff = True, squared = True)\n",
    "# plot_from_memory([1e-1, 1e1 * 2], local_input_dim, [\"neuron\"], [\"Wide\"],\n",
    "#                  scaleup = True, kernel_buff = True, squared = True)\n",
    "\n",
    "plot_from_memory([1e-3 * 0.5, 1e-1 * 2], local_input_dim, [\"smooth_neuron\"], [\"Wide\"],\n",
    "                 scaleup = False, kernel_buff = True, squared = True)\n",
    "plot_from_memory([1e-3 * 0.5, 1e-1 * 2], local_input_dim, [\"smooth_neuron\"], [\"Wide\"],\n",
    "                 scaleup = True, kernel_buff = True, squared = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_input_dim = 20\n",
    "\n",
    "# plot_from_memory([1e-3 * 0.5, 1e-1 * 2], local_input_dim, \n",
    "#                  [\"mean_flip\", \"median_flip\", \"maximum_flip\", \"softmax_flip\", \"second_flip\", \"potential\"],\n",
    "#                  [\"Wide\"], scaleup = False, kernel_buff = True, squared = True)\n",
    "\n",
    "# plot_from_memory([1e-0, 1e2], local_input_dim, [\"neuron\"], [\"Wide\"],\n",
    "#                  scaleup = False, kernel_buff = True, squared = True)\n",
    "\n",
    "\n",
    "plot_from_memory([1e-3 * 0.5, 1e-0], local_input_dim, [\"smooth_neuron\"], [\"Wide\"],\n",
    "                 scaleup = False, kernel_buff = True, squared = True)\n",
    "# plot_from_memory([1e-2, 1e0], local_input_dim, [\"smooth_neuron\"], [\"Wide\"],\n",
    "#                  scaleup = True, kernel_buff = True, squared = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_100 = [0.0794, 0.0763, 0.0802, 0.0766, 0.0892]\n",
    "s1_200 = [0.0535, 0.0577, 0.0608, 0.0542, 0.0546]\n",
    "s2_100 = [0.0829, 0.0897, 0.0827, 0.0754, 0.0816]\n",
    "s2_200 = [0.0510, 0.0548, 0.0586, 0.0688, 0.0560]\n",
    "s3_100 = [0.1459, 0.1419, 0.1393, 0.1507, 0.1449]\n",
    "s3_200 = [0.1084, 0.1045, 0.1052, 0.1150, 0.1069]\n",
    "\n",
    "def mean_std(x):\n",
    "    mean = np.mean(np.array(x)) * 100\n",
    "    std = np.std(np.array(x)) * 100\n",
    "    print(\"{:.2f} \\\\pm {:.2f}\".format(mean, std))\n",
    "\n",
    "mean_std(s1_100)\n",
    "mean_std(s1_200)\n",
    "mean_std(s2_100)\n",
    "mean_std(s2_200)\n",
    "mean_std(s3_100)\n",
    "mean_std(s3_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:prime] *",
   "language": "python",
   "name": "conda-env-prime-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
