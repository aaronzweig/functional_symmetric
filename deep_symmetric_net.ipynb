{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import itertools\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LittleBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LittleBlock, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.fc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Block, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim, bias = False)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F1: NN + NN\n",
    "class Symmetric(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_phi, hidden_dim_rho):\n",
    "        super(Symmetric, self).__init__()\n",
    "        \n",
    "        self.hidden_dim_phi = hidden_dim_phi\n",
    "        self.hidden_dim_rho = hidden_dim_rho\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.rho = None\n",
    "        self.phi = None\n",
    "        self.reinit()\n",
    "    \n",
    "    def reinit(self):\n",
    "        self.rho = Block(self.hidden_dim_phi, self.hidden_dim_rho, 1)\n",
    "        self.phi = LittleBlock(self.input_dim, self.hidden_dim_phi)\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        batch_size, input_set_dim, input_dim = x.shape\n",
    "        \n",
    "        x = x.view(-1, input_dim)\n",
    "        z = self.phi(x)\n",
    "        z = z.view(batch_size, input_set_dim, -1)\n",
    "        z = torch.mean(z, 1)\n",
    "        return self.rho(z)\n",
    "    \n",
    "    def regularize(self, lamb):\n",
    "        reg_loss = 0.\n",
    "        W1 = self.phi.fc.weight\n",
    "        W2 = self.rho.fc1.weight\n",
    "        w = self.rho.fc2.weight\n",
    "        \n",
    "        W1 = torch.norm(W1, dim = 1, keepdim = True)\n",
    "        W2 = torch.abs(W2)\n",
    "        w = torch.abs(w)\n",
    "        \n",
    "        reg_loss = torch.matmul(w, torch.matmul(W2, W1)).item()\n",
    "\n",
    "        return lamb * reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSets(Symmetric):\n",
    "    def __init__(self, input_dim, hidden_dim_phi, hidden_dim_rho):\n",
    "        super(DeepSets, self).__init__(input_dim, hidden_dim_phi, hidden_dim_rho)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        batch_size, input_set_dim, input_dim = x.shape\n",
    "        \n",
    "        x = x.view(-1, input_dim)\n",
    "        z = self.phi(x)\n",
    "        z = z.view(batch_size, input_set_dim, -1)\n",
    "        z = torch.sum(z, 1)\n",
    "        return self.rho(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F2: K + NN\n",
    "class KNN(Symmetric):\n",
    "    def __init__(self, input_dim, hidden_dim_phi, hidden_dim_rho):\n",
    "        super(KNN, self).__init__(input_dim, hidden_dim_phi, hidden_dim_rho)\n",
    "\n",
    "    def reinit(self):\n",
    "        super(KNN, self).reinit()\n",
    "        \n",
    "        self.phi.fc.weight.requires_grad = False\n",
    "        self.phi.fc.weight.div_(torch.norm(self.phi.fc.weight, dim = 1, keepdim = True))\n",
    "        \n",
    "    def regularize(self, lamb):\n",
    "        reg_loss = 0.\n",
    "\n",
    "        W2 = self.rho.fc1.weight\n",
    "        w = self.rho.fc2.weight\n",
    "        \n",
    "        W2 = torch.norm(W2, dim = 1, keepdim = True)\n",
    "        w = torch.abs(w)\n",
    "        \n",
    "        reg_loss = torch.matmul(w, W2).item()\n",
    "        \n",
    "        return lamb * reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F3: K + K\n",
    "class KK(KNN):\n",
    "    def __init__(self, input_dim, hidden_dim_phi, hidden_dim_rho):\n",
    "        super(KK, self).__init__(input_dim, hidden_dim_phi, hidden_dim_rho)\n",
    "\n",
    "    def reinit(self):\n",
    "        super(KK, self).reinit()\n",
    "        \n",
    "        self.rho.fc1.weight.requires_grad = False\n",
    "        self.rho.fc1.weight.div_(torch.norm(self.rho.fc1.weight, dim = 1, keepdim = True))\n",
    "        \n",
    "    def regularize(self, lamb):\n",
    "        reg_loss = 0.\n",
    "        \n",
    "        w = self.rho.fc2.weight\n",
    "\n",
    "        reg_loss = torch.norm(w)\n",
    "\n",
    "        return lamb * reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_narrow_data(N, batch_size, input_dim, objective):\n",
    "    x = np.random.uniform(low = -1, high = 1, size = (batch_size, N, input_dim))\n",
    "    y = objective(x)\n",
    "    \n",
    "    #bias term\n",
    "    x[:,:,-1] = 1\n",
    "    \n",
    "    x = torch.from_numpy(x).float()\n",
    "    y = torch.from_numpy(y).float()\n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wide_data(N, batch_size, input_dim, objective):\n",
    "    x = np.zeros((batch_size, N, input_dim))\n",
    "    for i in range(input_dim):\n",
    "        \n",
    "        a = np.random.uniform(low = -1, high = 1, size = batch_size)\n",
    "        b = np.random.uniform(low = -1, high = 1, size = batch_size)\n",
    "        a, b = np.minimum(a,b), np.maximum(a,b)\n",
    "\n",
    "        x_fill = np.random.uniform(low = np.tile(a, (N,1)), high = np.tile(b, (N,1)))\n",
    "        x[:,:,i] = x_fill.T\n",
    "    \n",
    "    y = objective(x)\n",
    "    \n",
    "    #bias term\n",
    "    x[:,:,-1] = 1\n",
    "    \n",
    "    x = torch.from_numpy(x).float()\n",
    "    y = torch.from_numpy(y).float()\n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(N, batch_size, input_dim, objective, narrow):\n",
    "    if narrow:\n",
    "        return generate_narrow_data(N, batch_size, input_dim, objective)\n",
    "    else:\n",
    "        return generate_wide_data(N, batch_size, input_dim, objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y, iterations, lamb = 0.1):\n",
    "    model.train()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "#     scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10)\n",
    "\n",
    "    indices = np.array_split(np.arange(x.shape[0]), x.shape[0]/20)\n",
    "\n",
    "    losses = []\n",
    "    for i in range(iterations):\n",
    "        index = indices[np.random.randint(len(indices))]\n",
    "        outputs = model(x[index])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, y[index])\n",
    "        loss += model.regularize(lamb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    model.eval()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalization_error(N_list, batch_size, model, objective, narrow):\n",
    "    errors = []\n",
    "    for N in N_list:\n",
    "        input_dim = model.input_dim\n",
    "        x,y = generate_data(N, batch_size, input_dim, objective, narrow)\n",
    "        outputs = model(x)\n",
    "        error = nn.MSELoss()(outputs, y).item()\n",
    "        errors.append(error)\n",
    "    return np.array(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = lambda x: np.mean(norm(x, axis = 2), axis = 1, keepdims = True)\n",
    "\n",
    "median = lambda x: np.median(norm(x, axis = 2), axis = 1, keepdims = True)\n",
    "\n",
    "maximum = lambda x: np.max(norm(x, axis = 2), axis = 1, keepdims = True)\n",
    "\n",
    "lamb = 0.1\n",
    "softmax = lambda x: lamb * np.log(np.mean(np.exp(norm(x, axis = 2) / lamb), axis = 1, keepdims = True))\n",
    "\n",
    "second = lambda x: np.sort(norm(x, axis = 2), axis = 1)[:,-2].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1110],\n",
      "        [ 0.0000],\n",
      "        [-0.1389],\n",
      "        [ 0.0000],\n",
      "        [-0.2638],\n",
      "        [ 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "### May need to sample several neurons to find one that isn't degenerate on the domain [-1,1]\n",
    "\n",
    "\n",
    "input_dim = 10\n",
    "teacher = Symmetric(input_dim, 50, 1)\n",
    "\n",
    "# torch.nn.init.normal_(teacher.phi.fc.weight,std = 1.)\n",
    "# torch.nn.init.normal_(teacher.rho.fc1.weight,std = 1.)\n",
    "# torch.nn.init.normal_(teacher.rho.fc2.weight,std = 1.)\n",
    "\n",
    "torch.nn.init.uniform_(teacher.phi.fc.weight, a = -10., b = 10.)\n",
    "torch.nn.init.uniform_(teacher.rho.fc1.weight,a = -10., b = 10.)\n",
    "# torch.nn.init.uniform_(teacher.rho.fc2.weight,a = -10., b = 10.)\n",
    "\n",
    "with torch.no_grad():\n",
    "    teacher.phi.fc.weight.div_(torch.mean(torch.norm(teacher.phi.fc.weight, dim = 1)))\n",
    "    teacher.rho.fc1.weight.div_(torch.mean(torch.norm(teacher.rho.fc1.weight, dim = 1)))\n",
    "\n",
    "teacher.eval()\n",
    "def neuron(x):\n",
    "    x = torch.from_numpy(x).float()\n",
    "    y = teacher(x)\n",
    "    return y.data.numpy().reshape(-1, 1)\n",
    "\n",
    "x, y = generate_narrow_data(3, 6, input_dim, neuron)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000],\n",
      "        [-0.0524],\n",
      "        [-0.0402],\n",
      "        [-0.0202],\n",
      "        [-0.0082],\n",
      "        [-0.0435]])\n"
     ]
    }
   ],
   "source": [
    "### May need to sample several neurons to find one that isn't degenerate on the domain [-1,1]\n",
    "\n",
    "input_dim = 10\n",
    "smooth_teacher = Symmetric(input_dim, 50, 1)\n",
    "\n",
    "# torch.nn.init.normal_(teacher.rho.fc1.weight,std = 1.)\n",
    "# torch.nn.init.normal_(teacher.rho.fc2.weight,std = 1.)\n",
    "\n",
    "torch.nn.init.uniform_(teacher.rho.fc1.weight,a = -2., b = 2.)\n",
    "torch.nn.init.uniform_(teacher.rho.fc2.weight,a = -2., b = 2.)\n",
    "\n",
    "with torch.no_grad():\n",
    "    teacher.phi.fc.weight.div_(torch.mean(torch.norm(teacher.phi.fc.weight, dim = 1)))\n",
    "    teacher.rho.fc1.weight.div_(torch.mean(torch.norm(teacher.rho.fc1.weight, dim = 1)))\n",
    "\n",
    "smooth_teacher.eval()\n",
    "def smooth_neuron(x):\n",
    "    x = torch.from_numpy(x).float()\n",
    "    y = smooth_teacher(x)\n",
    "    return y.data.numpy().reshape(-1, 1)\n",
    "\n",
    "x, y = generate_narrow_data(3, 6, input_dim, smooth_neuron)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron.__name__ = \"neuron\"\n",
    "smooth_neuron.__name__ = \"smooth_neuron\"\n",
    "maximum.__name__ = \"maximum\"\n",
    "softmax.__name__ = \"softmax\"\n",
    "median.__name__ = \"median\"\n",
    "mean.__name__ = \"mean\"\n",
    "second.__name__ = \"second\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(model, x, y, iterations, lambs, verbose):\n",
    "    models = []\n",
    "    for lamb in lambs:\n",
    "        model_copy = copy.deepcopy(model)\n",
    "        losses = train(model_copy, x, y, iterations, lamb)\n",
    "        models.append(model_copy)\n",
    "        if verbose and lamb == 0:\n",
    "            print(\"check for overfitting power of\", model.__name__)\n",
    "            print(losses[::int(iterations/10)])\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, objective, narrow, verbose = True, log_plot = False):\n",
    "#     x, y = generate_data(N_max, batch_size, input_dim, objective, narrow)\n",
    "    print(\"currently\", objective.__name__)\n",
    "        \n",
    "    f1 = Symmetric(input_dim, 1000, hidden_dim)\n",
    "    f2 = KNN(input_dim, 1000, hidden_dim)\n",
    "    f3 = KK(input_dim, 1000, 1000)\n",
    "\n",
    "    f1.__name__ = \"S1\"\n",
    "    f2.__name__ = \"S2\"\n",
    "    f3.__name__ = \"S3\"\n",
    "\n",
    "    models = [f1, f2, f3]\n",
    "    \n",
    "    lambs = [0., 1e-6, 1e-4, 1e-2, 1]\n",
    "    N_list = np.arange(2, N_max + 16)\n",
    "\n",
    "    for model in models:\n",
    "        x, y = generate_data(N_max, batch_size, input_dim, objective, narrow)\n",
    "        cv_models = cross_validate(model, x, y, iterations, lambs, verbose)\n",
    "        \n",
    "        validation_errors = np.zeros_like(lambs)\n",
    "        for i, cv_model in enumerate(cv_models):\n",
    "            validation_errors[i] = generalization_error([N_max], 1000, cv_model, objective, narrow)[0]\n",
    "        \n",
    "        i = np.argmin(validation_errors)\n",
    "        lamb = lambs[i]\n",
    "            \n",
    "        runs = 10\n",
    "        run_errors = np.zeros((runs, len(N_list)))\n",
    "        for i in range(runs):\n",
    "            x, y = generate_data(N_max, batch_size, input_dim, objective, narrow)\n",
    "            model_copy = copy.deepcopy(model)\n",
    "            model_copy.reinit()\n",
    "            train(model_copy, x, y, iterations, lamb)\n",
    "            errors = generalization_error(N_list, 1000, model_copy, objective, narrow)\n",
    "            run_errors[i] = np.array(errors)\n",
    "        \n",
    "        mean_error = np.mean(run_errors, axis = 0)\n",
    "        std_error = np.std(run_errors, axis = 0)\n",
    "        if log_plot:\n",
    "            plt.semilogy(N_list, mean_error, label = model.__name__)\n",
    "        else:\n",
    "            plt.plot(N_list, mean_error, label = model.__name__)\n",
    "        plt.fill_between(N_list, mean_error - std_error, mean_error + std_error, alpha = 0.2)\n",
    "\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.ylim([1e-5, 1e-1]) \n",
    "    plt.xlabel(\"N\")\n",
    "    plt.ylabel(\"Mean Square Error\")\n",
    "    narrow_str = \"Narrow\" if narrow else \"Wide\"\n",
    "    plt.title(narrow_str + \" generalization for \" + objective.__name__)\n",
    "    plt.savefig(\"plots_high_dim/\" + objective.__name__ + \"_\" + narrow_str + \"_\" + str(input_dim))\n",
    "#     plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run to generate plots in Figure 1:\n",
    "\n",
    "N_max = 4\n",
    "hidden_dim = 50\n",
    "\n",
    "iterations = 1000\n",
    "batch_size = 100\n",
    "\n",
    "input_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently mean\n",
      "check for overfitting power\n",
      "[2.3047752380371094, 0.002783059375360608, 0.00017399639182258397, 0.0002541202702559531, 0.0002493889187462628, 0.00045954054803587496, 3.897277565556578e-05, 1.0654335710569285e-05, 9.75105467659887e-06, 2.39704386331141e-05]\n",
      "check for overfitting power\n",
      "[2.2417304515838623, 0.002252849517390132, 0.0009137581218965352, 0.001335066626779735, 0.0011182911694049835, 0.0006375341326929629, 0.0005794171593151987, 0.0006989827379584312, 0.00013785649207420647, 6.958266749279574e-05]\n",
      "check for overfitting power\n",
      "[1.1356312036514282, 0.0015773542691022158, 0.0011178527493029833, 0.00048722411156632006, 0.0006756122456863523, 0.00025430653477087617, 5.262292688712478e-05, 4.309754149289802e-05, 1.506218086433364e-05, 0.00028825673507526517]\n",
      "currently mean\n",
      "check for overfitting power\n",
      "[3.406557083129883, 0.0008517092210240662, 0.0010509208077564836, 0.00022463369532488286, 5.358149064704776e-05, 4.3858704884769395e-05, 1.2801954653696157e-05, 1.213838822877733e-05, 5.586753104580566e-06, 2.1481234853126807e-06]\n",
      "check for overfitting power\n",
      "[3.4053313732147217, 0.002771230647340417, 0.0006687982240691781, 0.0005629378720186651, 0.0003073982079513371, 0.00042722374200820923, 0.00023436654009856284, 0.0003585749363992363, 0.00020032103930134326, 0.00015144811186473817]\n",
      "check for overfitting power\n",
      "[3.4565346240997314, 0.003542531281709671, 0.0019526388496160507, 0.0016575524350628257, 0.0006104019703343511, 0.0004206199664622545, 0.00018890798673965037, 0.00013330820365808904, 4.251544669386931e-05, 6.978909368626773e-05]\n",
      "currently median\n",
      "check for overfitting power\n",
      "[2.1844534873962402, 0.0012811371125280857, 0.0006911529344506562, 0.00023214766406454146, 0.00020636086992453784, 5.459871681523509e-05, 3.2876407203730196e-05, 6.446035513363313e-06, 1.0722620800152072e-06, 4.178139533905778e-06]\n",
      "check for overfitting power\n",
      "[1.928550124168396, 0.0014788502594456077, 0.002923604566603899, 0.0008363152737729251, 0.000812630751170218, 0.001293632318265736, 0.0005343016237020493, 0.0003685301635414362, 0.00010703494626795873, 8.882612746674567e-05]\n",
      "check for overfitting power\n",
      "[2.302121639251709, 0.008141180500388145, 0.0030517985578626394, 0.0008621808374300599, 0.0005553318769671023, 0.0009082372416742146, 0.0002593319513835013, 0.00014740796177648008, 6.596323510166258e-05, 3.500327147776261e-05]\n",
      "currently median\n",
      "check for overfitting power\n",
      "[3.3863863945007324, 0.0021894914098083973, 0.0006011639488860965, 0.00040398334385827184, 0.00017523099086247385, 9.896208212012425e-05, 9.437288099434227e-06, 1.6798258002381772e-06, 7.452616728187422e-07, 2.8747913916049583e-07]\n",
      "check for overfitting power\n",
      "[3.3166725635528564, 0.00633955979719758, 0.0032560359686613083, 0.005097072571516037, 0.0029263149481266737, 0.005810413043946028, 0.0018167456146329641, 0.0009103025076910853, 0.0005334450979717076, 0.0017285728827118874]\n",
      "check for overfitting power\n",
      "[3.5084006786346436, 0.007754589430987835, 0.003677037777379155, 0.0027021318674087524, 0.001242118189111352, 0.00045914523070678115, 0.00024286664847750217, 0.00044201166019774973, 0.00018269653082825243, 8.970091585069895e-05]\n",
      "currently maximum\n",
      "check for overfitting power\n",
      "[2.66462779045105, 0.006930856499820948, 0.0033406249713152647, 0.0012054244289174676, 0.0008867021533660591, 0.0002679173194337636, 0.00016963908274192363, 2.1983916667522863e-05, 2.8918093448737636e-05, 8.198565410566516e-06]\n",
      "check for overfitting power\n",
      "[2.5029280185699463, 0.00983507838100195, 0.003078673267737031, 0.0025718894321471453, 0.0032745946664363146, 0.0015475609106943011, 0.0017338140169158578, 0.0007898781332187355, 0.0003350716142449528, 0.00015323991829063743]\n",
      "check for overfitting power\n",
      "[2.4201290607452393, 0.004997921641916037, 0.0035394460428506136, 0.0023848158307373524, 0.0005325512611307204, 0.0009923361940309405, 0.0002251659461762756, 0.00037247996078804135, 0.00013729686907026917, 0.00012709082511719316]\n",
      "currently maximum\n",
      "check for overfitting power\n",
      "[4.430531024932861, 0.011866817250847816, 0.0036820019595324993, 0.0035963673144578934, 0.002491272985935211, 0.00042394045158289373, 0.0003509267116896808, 0.00027484624297358096, 0.0052689118310809135, 0.00030103049357421696]\n",
      "check for overfitting power\n",
      "[4.485527992248535, 0.008757630363106728, 0.005918608512729406, 0.004343782085925341, 0.005176376551389694, 0.0027363765984773636, 0.001239798846654594, 0.0010073708835989237, 0.0011675124987959862, 0.002606602618470788]\n",
      "check for overfitting power\n",
      "[4.391333103179932, 0.015891510993242264, 0.009609926491975784, 0.004964365158230066, 0.003921434748917818, 0.0028129923157393932, 0.003122169291600585, 0.002011731266975403, 0.0008449826273135841, 0.0007107107667252421]\n",
      "currently softmax\n",
      "check for overfitting power\n",
      "[2.2856736183166504, 0.0025223898701369762, 0.0011108461767435074, 0.00022418765001930296, 0.00011285749496892095, 2.907457201217767e-05, 1.772357245499734e-05, 2.986758318002103e-06, 2.8541472829601844e-07, 1.0237922509759301e-07]\n",
      "check for overfitting power\n",
      "[2.4683899879455566, 0.004884373862296343, 0.0033999974839389324, 0.004960470367223024, 0.0011091965716332197, 0.0007474023150280118, 0.00036437157541513443, 0.0003902344615198672, 0.00038703580503351986, 0.0008199635776691139]\n",
      "check for overfitting power\n",
      "[1.9285205602645874, 0.0027602333575487137, 0.0027225459925830364, 0.0021630830597132444, 0.0018258246127516031, 0.00034553560544736683, 0.001316466135904193, 0.0001912986335810274, 0.00018718351202551275, 2.3452013920177706e-05]\n",
      "currently softmax\n",
      "check for overfitting power\n",
      "[3.8848800659179688, 0.005310738924890757, 0.0031748723704367876, 0.0013492244761437178, 0.0003979040193371475, 0.0006838766857981682, 0.000288451585220173, 0.00012920223525725305, 3.740698957699351e-05, 2.3900454834802076e-05]\n",
      "check for overfitting power\n",
      "[3.7880616188049316, 0.00805600918829441, 0.003652102779597044, 0.00359175493940711, 0.0018251644214615226, 0.002271356526762247, 0.0011406620033085346, 0.0015814978396520019, 0.0007373234839178622, 0.0004168235172983259]\n",
      "check for overfitting power\n",
      "[3.0754940509796143, 0.01111625600606203, 0.0028324637096375227, 0.002519491594284773, 0.0008725548395887017, 0.0013732804218307137, 0.000875971163623035, 0.00017627034685574472, 0.00022569741122424603, 0.0001796087744878605]\n",
      "currently second\n",
      "check for overfitting power\n",
      "[2.1642253398895264, 0.0035674148239195347, 0.000722808763384819, 0.0004047541006002575, 8.262322808150202e-05, 7.849904068280011e-05, 2.5611079763621092e-05, 7.236129022203386e-06, 5.471963504533051e-06, 1.276889406653936e-06]\n",
      "check for overfitting power\n",
      "[2.339773654937744, 0.004369495436549187, 0.0014507184969261289, 0.0012977260630577803, 0.0008872895268723369, 0.0009963028132915497, 0.00014553466462530196, 0.0009959203889593482, 5.670414975611493e-05, 0.00012887382763437927]\n",
      "check for overfitting power\n",
      "[1.9125076532363892, 0.005463828798383474, 0.004536087159067392, 0.0010928490664809942, 0.0009865174070000648, 0.0003825035528279841, 0.00019688604515977204, 0.00017011242744047195, 7.45604993426241e-05, 7.984271360328421e-05]\n",
      "currently second\n",
      "check for overfitting power\n",
      "[3.8851280212402344, 0.0039684162475168705, 0.0043795108795166016, 0.001574740163050592, 0.0005693462444469333, 0.00021908190683461726, 3.703998663695529e-05, 2.9450962756527588e-05, 6.278407909121597e-06, 9.336883408650465e-07]\n",
      "check for overfitting power\n",
      "[3.5522894859313965, 0.009277512319386005, 0.004598674364387989, 0.003375314874574542, 0.007479500956833363, 0.001615532673895359, 0.0014940797118470073, 0.0011949560139328241, 0.0008908884483389556, 0.0011644941987469792]\n",
      "check for overfitting power\n",
      "[4.146859169006348, 0.010763034224510193, 0.0036624656058847904, 0.005004121921956539, 0.0007300282595679164, 0.000551955308765173, 0.0007031728746369481, 0.0004938658676110208, 0.00021801705588586628, 4.666571840061806e-05]\n",
      "currently neuron\n",
      "check for overfitting power\n",
      "[0.009805603884160519, 0.0005166589980944991, 5.611923916148953e-05, 0.0003541891637723893, 0.00021612159616779536, 0.0002043303829850629, 0.00020307270460762084, 1.0102400665346067e-05, 5.52694018551847e-06, 8.174795948434621e-05]\n",
      "check for overfitting power\n",
      "[0.028154190629720688, 0.0008906177245080471, 2.8582642698893324e-05, 1.509274443378672e-05, 1.817089105315972e-05, 0.00018692121375352144, 0.0004915267345495522, 0.00019110199355054647, 6.882429261167999e-06, 3.6092885693506105e-06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check for overfitting power\n",
      "[0.04113614559173584, 0.0004033070872537792, 0.0002051381452474743, 0.00011936268856516108, 0.00045927875908091664, 0.0049672857858240604, 0.0025650514289736748, 0.00012907215568702668, 1.5132117368921172e-05, 0.00019784728647209704]\n",
      "currently neuron\n",
      "check for overfitting power\n",
      "[0.003397454973310232, 0.0006860502762719989, 0.0004577323270495981, 0.00037512482958845794, 3.286478022346273e-05, 0.00019349031208548695, 1.674845589150209e-05, 3.9130485674832016e-05, 1.106493527913699e-06, 1.0388267668304252e-07]\n",
      "check for overfitting power\n",
      "[0.004624880850315094, 0.0025437786243855953, 0.0, 0.010122809559106827, 0.0, 0.0019508779514580965, 0.0, 0.0016945298993960023, 0.010122809559106827, 0.0019508779514580965]\n",
      "check for overfitting power\n",
      "[0.03023776412010193, 0.0006069460650905967, 0.00015993285342119634, 3.809927511611022e-05, 9.478800166107249e-06, 0.0009227569098584354, 6.551707338076085e-05, 8.316764024129952e-07, 1.836461524362676e-05, 0.00016059548943303525]\n",
      "currently smooth_neuron\n",
      "check for overfitting power\n",
      "[0.001286125392653048, 0.0010269859340041876, 0.0010269859340041876, 0.0024823429994285107, 0.0020046010613441467, 0.0010269859340041876, 0.0020046010613441467, 0.0024823429994285107, 0.0019068133551627398, 0.0010269859340041876]\n",
      "check for overfitting power\n",
      "[0.003039918141439557, 0.0002682821359485388, 0.00015423519653268158, 6.073887925595045e-05, 4.730392902274616e-05, 3.498566002235748e-05, 2.1020856365794316e-05, 2.0513585695880465e-05, 3.267553620389663e-05, 2.7877260890818434e-06]\n",
      "check for overfitting power\n",
      "[0.01837894134223461, 7.178369560278952e-05, 6.38897399767302e-05, 1.8007209291681647e-05, 1.0371220923843794e-05, 0.00015626881213393062, 2.4912962544476613e-05, 0.0005856740172021091, 0.0025350586511194706, 0.0013221284607425332]\n",
      "currently smooth_neuron\n",
      "check for overfitting power\n",
      "[0.003115569707006216, 0.0011986794415861368, 0.0011986794415861368, 0.0011986794415861368, 0.0005443845293484628, 0.0011986794415861368, 0.0005443845293484628, 0.0005365522811189294, 0.0010694640222936869, 0.0010694640222936869]\n",
      "check for overfitting power\n",
      "[0.002296384423971176, 0.0005706952069886029, 0.0006271990132518113, 0.00033245939994230866, 0.0005706952069886029, 0.0006271990132518113, 0.0005706952069886029, 0.0005706952069886029, 0.0005706952069886029, 0.0006271990132518113]\n",
      "check for overfitting power\n",
      "[0.010218625888228416, 0.00012031015648972243, 2.8832135285483673e-05, 3.793580617639236e-06, 6.195317041601811e-07, 2.3387369196825603e-07, 9.674518963720402e-08, 4.510743778496362e-08, 8.350320968020242e-08, 3.625780777838372e-07]\n"
     ]
    }
   ],
   "source": [
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, mean, narrow = False, log_plot = True)\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim , mean, narrow = True, log_plot = True)\n",
    "\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, median, narrow = False, log_plot = True)\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim , median, narrow = True, log_plot = True)\n",
    "\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, maximum, narrow = False, log_plot = True)\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim , maximum, narrow = True, log_plot = True)\n",
    "\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, softmax, narrow = False, log_plot = True)\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim , softmax, narrow = True, log_plot = True)\n",
    "\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, second, narrow = False, log_plot = True)\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim , second, narrow = True, log_plot = True)\n",
    "\n",
    "compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, neuron, narrow = False, log_plot = True)\n",
    "compare_models(N_max, hidden_dim, iterations, batch_size, input_dim , neuron, narrow = True, log_plot = True)\n",
    "\n",
    "compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, smooth_neuron, narrow = False, log_plot = True)\n",
    "compare_models(N_max, hidden_dim, iterations, batch_size, input_dim , smooth_neuron, narrow = True, log_plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots for right half of Figure 2\n",
    "\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim , smooth_neuron, narrow = True, log_plot = True)\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim , neuron, narrow = True, log_plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.011131564155220985, 0.0014981222338974476, 0.0005335101159289479, 0.0005359654896892607, 0.0005274279974400997, 0.0005361093790270388, 0.0005384979886002839, 0.0005354254972189665, 0.0005588674684986472, 0.0005240787868387997]\n",
      "f1 [0.00656719]\n",
      "[0.06285697966814041, 0.013469170778989792, 0.01869463175535202, 0.01869463175535202, 0.013469170778989792, 0.030276844277977943, 0.008029865100979805, 0.013469170778989792, 0.0118960440158844, 0.013469170778989792]\n",
      "f2 [0.01477099]\n",
      "[0.009445526637136936, 0.0003498205041978508, 0.001232254900969565, 0.0004458093026187271, 5.454664278659038e-05, 0.00011973125947406515, 2.8050522814737633e-05, 0.00255468743853271, 0.0023875662591308355, 0.0016647777520120144]\n",
      "f3 [0.00882316]\n",
      "[0.0073065925389528275, 0.001781738130375743, 0.0006876902189105749, 0.0006504563498310745, 0.0006505273631773889, 0.0006528985104523599, 0.0006745976861566305, 0.0007234070799313486, 0.0006582402274943888, 0.0006518846494145691]\n",
      "f1 [0.00631934]\n"
     ]
    }
   ],
   "source": [
    "N_max = 4\n",
    "hidden_dim = 50\n",
    "iterations = 2000\n",
    "batch_size = 100\n",
    "\n",
    "objective = neuron\n",
    "narrow = False\n",
    "\n",
    "input_dim = 10\n",
    "\n",
    "\n",
    "x, y = generate_data(N_max, batch_size, input_dim, objective, narrow)\n",
    "\n",
    "for i in range(20):\n",
    "    model = Symmetric(input_dim, 2000, hidden_dim)\n",
    "    model.train()\n",
    "    losses = train(model, x, y, iterations, lamb = 0.00001)\n",
    "    model.eval()\n",
    "    print(losses[::int(iterations/10)])\n",
    "    print(\"f1\", generalization_error([4], 5000, model, objective, narrow))\n",
    "    \n",
    "    model = KNN(input_dim, 2000, hidden_dim)\n",
    "    model.train()\n",
    "    losses = train(model, x, y, iterations, lamb = 0.00001)\n",
    "    model.eval()\n",
    "    print(losses[::int(iterations/10)])\n",
    "    print(\"f2\", generalization_error([4], 5000, model, objective, narrow))\n",
    "    \n",
    "    model = KK(input_dim, 2000, 1000)\n",
    "    model.train()\n",
    "    losses = train(model, x, y, iterations, lamb = 0.00001)\n",
    "    model.eval()\n",
    "    print(losses[::int(iterations/10)])\n",
    "    print(\"f3\", generalization_error([4], 5000, model, objective, narrow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for Figure 3\n",
    "\n",
    "# N_max = 4\n",
    "# hidden_dim = 50\n",
    "\n",
    "# iterations = 1000\n",
    "# batch_size = 100\n",
    "\n",
    "# input_dim = 5\n",
    "\n",
    "# objective = mean\n",
    "# narrow = False\n",
    "\n",
    "# log_plot = True\n",
    "\n",
    "# f1 = Symmetric(input_dim, 1000, hidden_dim)\n",
    "# f2 = DeepSets(input_dim, 1000, hidden_dim)\n",
    "# f1.__name__ = \"S1\"\n",
    "# f2.__name__ = \"DeepSets\"\n",
    "\n",
    "# models = [f1, f2]\n",
    "\n",
    "# N_list = np.arange(2, N_max + 16)\n",
    "\n",
    "# for model in models:\n",
    "#     x, y = generate_data(N_max, batch_size, input_dim, objective, narrow)\n",
    "\n",
    "#     lamb = 0.\n",
    "\n",
    "#     runs = 10\n",
    "#     run_errors = np.zeros((runs, len(N_list)))\n",
    "#     for i in range(runs):\n",
    "#         x, y = generate_data(N_max, batch_size, input_dim, objective, narrow)\n",
    "#         model_copy = copy.deepcopy(model)\n",
    "#         model_copy.reinit()\n",
    "#         train(model_copy, x, y, iterations, lamb)\n",
    "#         errors = generalization_error(N_list, 1000, model_copy, objective, narrow)\n",
    "#         run_errors[i] = np.array(errors)\n",
    "\n",
    "#     mean_error = np.mean(run_errors, axis = 0)\n",
    "#     std_error = np.std(run_errors, axis = 0)\n",
    "#     if log_plot:\n",
    "#         plt.semilogy(N_list, mean_error, label = model.__name__)\n",
    "#     else:\n",
    "#         plt.plot(N_list, mean_error, label = model.__name__)\n",
    "#     plt.fill_between(N_list, mean_error - std_error, mean_error + std_error, alpha = 0.2)\n",
    "\n",
    "\n",
    "# plt.legend()\n",
    "# #     plt.ylim([1e-5, 1e10]) \n",
    "# plt.xlabel(\"N\")\n",
    "# plt.ylabel(\"Mean Square Error\")\n",
    "# narrow_str = \"Narrow\" if narrow else \"Wide\"\n",
    "# plt.title(\"Normalized vs. Unnormalized generalization for \" + objective.__name__)\n",
    "# plt.savefig(\"plots_high_dim/\" + \"deepsets\")\n",
    "# #     plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
