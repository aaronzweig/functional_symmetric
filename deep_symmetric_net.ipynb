{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import itertools\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LittleBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LittleBlock, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.fc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Block, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim, bias = False)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F1: NN + NN\n",
    "class Symmetric(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_phi, hidden_dim_rho):\n",
    "        super(Symmetric, self).__init__()\n",
    "        \n",
    "        self.hidden_dim_phi = hidden_dim_phi\n",
    "        self.hidden_dim_rho = hidden_dim_rho\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.rho = None\n",
    "        self.phi = None\n",
    "        self.reinit()\n",
    "    \n",
    "    def reinit(self):\n",
    "        self.rho = Block(self.hidden_dim_phi, self.hidden_dim_rho, 1)\n",
    "        self.phi = LittleBlock(self.input_dim, self.hidden_dim_phi)\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        batch_size, input_set_dim, input_dim = x.shape\n",
    "        \n",
    "        x = x.view(-1, input_dim)\n",
    "        z = self.phi(x)\n",
    "        z = z.view(batch_size, input_set_dim, -1)\n",
    "        z = torch.mean(z, 1)\n",
    "        return self.rho(z)\n",
    "    \n",
    "    def regularize(self, lamb):\n",
    "        reg_loss = 0.\n",
    "        W1 = self.phi.fc.weight\n",
    "        W2 = self.rho.fc1.weight\n",
    "        w = self.rho.fc2.weight\n",
    "        \n",
    "        W1 = torch.norm(W1, dim = 1, keepdim = True)\n",
    "        W2 = torch.abs(W2)\n",
    "        w = torch.abs(w)\n",
    "        \n",
    "        reg_loss = torch.matmul(w, torch.matmul(W2, W1)).item()\n",
    "\n",
    "        return lamb * reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSets(Symmetric):\n",
    "    def __init__(self, input_dim, hidden_dim_phi, hidden_dim_rho):\n",
    "        super(DeepSets, self).__init__(input_dim, hidden_dim_phi, hidden_dim_rho)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        batch_size, input_set_dim, input_dim = x.shape\n",
    "        \n",
    "        x = x.view(-1, input_dim)\n",
    "        z = self.phi(x)\n",
    "        z = z.view(batch_size, input_set_dim, -1)\n",
    "        z = torch.sum(z, 1)\n",
    "        return self.rho(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F2: K + NN\n",
    "class KNN(Symmetric):\n",
    "    def __init__(self, input_dim, hidden_dim_phi, hidden_dim_rho):\n",
    "        super(KNN, self).__init__(input_dim, hidden_dim_phi, hidden_dim_rho)\n",
    "\n",
    "    def reinit(self):\n",
    "        super(KNN, self).reinit()\n",
    "        \n",
    "        self.phi.fc.weight.requires_grad = False\n",
    "        self.phi.fc.bias.requires_grad = False\n",
    "        \n",
    "        self.phi.fc.weight.div_(torch.norm(self.phi.fc.weight, dim = 1, keepdim = True))\n",
    "        \n",
    "    def regularize(self, lamb):\n",
    "        reg_loss = 0.\n",
    "\n",
    "        W2 = self.rho.fc1.weight\n",
    "        w = self.rho.fc2.weight\n",
    "        \n",
    "        W2 = torch.norm(W2, dim = 1, keepdim = True)\n",
    "        w = torch.abs(w)\n",
    "        \n",
    "        reg_loss = torch.matmul(w, W2).item()\n",
    "        \n",
    "        return lamb * reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F3: K + K\n",
    "class KK(KNN):\n",
    "    def __init__(self, input_dim, hidden_dim_phi, hidden_dim_rho):\n",
    "        super(KK, self).__init__(input_dim, hidden_dim_phi, hidden_dim_rho)\n",
    "\n",
    "    def reinit(self):\n",
    "        super(KK, self).reinit()\n",
    "        \n",
    "        self.rho.fc1.weight.requires_grad = False\n",
    "        self.rho.fc1.bias.requires_grad = False\n",
    "        \n",
    "        self.rho.fc1.weight.div_(torch.norm(self.rho.fc1.weight, dim = 1, keepdim = True))\n",
    "\n",
    "        \n",
    "    def regularize(self, lamb):\n",
    "        reg_loss = 0.\n",
    "        \n",
    "        w = self.rho.fc2.weight\n",
    "\n",
    "        reg_loss = torch.norm(w)\n",
    "\n",
    "        return lamb * reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_narrow_data(N, batch_size, input_dim, objective):\n",
    "    x = np.random.uniform(low = -1, high = 1, size = (batch_size, N, input_dim))\n",
    "    y = objective(x)\n",
    "    \n",
    "    x = torch.from_numpy(x).float()\n",
    "    y = torch.from_numpy(y).float()\n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wide_data(N, batch_size, input_dim, objective):\n",
    "    x = np.zeros((batch_size, N, input_dim))\n",
    "    for i in range(input_dim):\n",
    "        \n",
    "        a = np.random.uniform(low = -1, high = 1, size = batch_size)\n",
    "        b = np.random.uniform(low = -1, high = 1, size = batch_size)\n",
    "        a, b = np.minimum(a,b), np.maximum(a,b)\n",
    "\n",
    "        x_fill = np.random.uniform(low = np.tile(a, (N,1)), high = np.tile(b, (N,1)))\n",
    "        x[:,:,i] = x_fill.T\n",
    "    \n",
    "    y = objective(x)\n",
    "    \n",
    "    x = torch.from_numpy(x).float()\n",
    "    y = torch.from_numpy(y).float()\n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(N, batch_size, input_dim, objective, narrow):\n",
    "    if narrow:\n",
    "        return generate_narrow_data(N, batch_size, input_dim, objective)\n",
    "    else:\n",
    "        return generate_wide_data(N, batch_size, input_dim, objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y, iterations, lamb = 0.1):\n",
    "    model.train()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "#     scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10)\n",
    "\n",
    "    indices = np.array_split(np.arange(x.shape[0]), x.shape[0]/20)\n",
    "\n",
    "    losses = []\n",
    "    for i in range(iterations):\n",
    "        index = indices[np.random.randint(len(indices))]\n",
    "        outputs = model(x[index])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, y[index])\n",
    "        loss += model.regularize(lamb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    model.eval()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalization_error(N_list, batch_size, model, objective, narrow):\n",
    "    errors = []\n",
    "    for N in N_list:\n",
    "        input_dim = model.input_dim\n",
    "        x,y = generate_data(N, batch_size, input_dim, objective, narrow)\n",
    "        outputs = model(x)\n",
    "        error = nn.MSELoss()(outputs, y).item()\n",
    "        errors.append(error)\n",
    "    return np.array(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = lambda x: np.mean(norm(x, axis = 2), axis = 1, keepdims = True)\n",
    "\n",
    "median = lambda x: np.median(norm(x, axis = 2), axis = 1, keepdims = True)\n",
    "\n",
    "maximum = lambda x: np.max(norm(x, axis = 2), axis = 1, keepdims = True)\n",
    "\n",
    "lamb = 0.1\n",
    "softmax = lambda x: lamb * np.log(np.mean(np.exp(norm(x, axis = 2) / lamb), axis = 1, keepdims = True))\n",
    "\n",
    "second = lambda x: np.sort(norm(x, axis = 2), axis = 1)[:,-2].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.9779],\n",
      "        [ -9.3473],\n",
      "        [-11.8528],\n",
      "        [ -5.6205]])\n"
     ]
    }
   ],
   "source": [
    "### May need to sample several neurons to find one that isn't degenerate on the domain [-1,1]\n",
    "\n",
    "###NOTE: when you come back to this in a few months, you may find S1 isn't doing as well\n",
    "# on the neuron since we fixed the path norm regularization.  Consider a wackier weight\n",
    "# distribution to make S2 and S3 even worse\n",
    "\n",
    "input_dim = 10\n",
    "teacher = Symmetric(input_dim, 1, 1)\n",
    "\n",
    "torch.nn.init.normal_(teacher.phi.fc.weight,std = 1.)\n",
    "torch.nn.init.normal_(teacher.rho.fc1.weight,std = 1.)\n",
    "torch.nn.init.normal_(teacher.rho.fc2.weight,std = 1.)\n",
    "\n",
    "# torch.nn.init.normal_(teacher.phi.fc.weight,std = 3.)\n",
    "# torch.nn.init.normal_(teacher.rho.fc1.weight,std = 3.)\n",
    "# torch.nn.init.normal_(teacher.rho.fc2.weight,std = 3.)\n",
    "\n",
    "teacher.eval()\n",
    "def neuron(x):\n",
    "    x = torch.from_numpy(x).float()\n",
    "    y = teacher(x)\n",
    "    return y.data.numpy().reshape(-1, 1)\n",
    "\n",
    "x, y = generate_narrow_data(3, 4, input_dim, neuron)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1887],\n",
      "        [-0.2303],\n",
      "        [-0.1945],\n",
      "        [-0.2155]])\n"
     ]
    }
   ],
   "source": [
    "### May need to sample several neurons to find one that isn't degenerate on the domain [-1,1]\n",
    "\n",
    "input_dim = 10\n",
    "smooth_teacher = Symmetric(input_dim, 1, 1)\n",
    "\n",
    "smooth_teacher.eval()\n",
    "def smooth_neuron(x):\n",
    "    x = torch.from_numpy(x).float()\n",
    "    y = smooth_teacher(x)\n",
    "    return y.data.numpy().reshape(-1, 1)\n",
    "\n",
    "x, y = generate_narrow_data(3, 4, input_dim, smooth_neuron)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron.__name__ = \"neuron\"\n",
    "smooth_neuron.__name__ = \"smooth_neuron\"\n",
    "maximum.__name__ = \"maximum\"\n",
    "softmax.__name__ = \"softmax\"\n",
    "median.__name__ = \"median\"\n",
    "mean.__name__ = \"mean\"\n",
    "second.__name__ = \"second\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(model, x, y, iterations, lambs, verbose):\n",
    "    models = []\n",
    "    for lamb in lambs:\n",
    "        model_copy = copy.deepcopy(model)\n",
    "        losses = train(model_copy, x, y, iterations, lamb)\n",
    "        models.append(model_copy)\n",
    "        if verbose:\n",
    "            print(losses[::int(iterations/10)])\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, objective, narrow, verbose = False, log_plot = False, scaleup = False):\n",
    "#     x, y = generate_data(N_max, batch_size, input_dim, objective, narrow)\n",
    "    print(\"currently\", objective.__name__)\n",
    "    \n",
    "    c = 1 if not scaleup else 2\n",
    "    \n",
    "    f1 = Symmetric(input_dim, 1000, hidden_dim)\n",
    "    f2 = KNN(input_dim, c * 1000, hidden_dim)\n",
    "    f3 = KK(input_dim, c * 1000, 1000)\n",
    "\n",
    "    f1.__name__ = \"S1\"\n",
    "    f2.__name__ = \"S2\"\n",
    "    f3.__name__ = \"S3\"\n",
    "\n",
    "    models = [f1, f2, f3]\n",
    "    \n",
    "    lambs = [0., 1e-6, 1e-4, 1e-2]\n",
    "    N_list = np.arange(2, N_max + 16)\n",
    "\n",
    "    for model in models:\n",
    "        x, y = generate_data(N_max, batch_size, input_dim, objective, narrow)\n",
    "        cv_models = cross_validate(model, x, y, iterations, lambs, verbose)\n",
    "        \n",
    "        validation_errors = np.zeros_like(lambs)\n",
    "        for i, cv_model in enumerate(cv_models):\n",
    "            validation_errors[i] = generalization_error([N_max], 1000, cv_model, objective, narrow)[0]\n",
    "        \n",
    "        i = np.argmin(validation_errors)\n",
    "        lamb = lambs[i]\n",
    "            \n",
    "        runs = 10\n",
    "        run_errors = np.zeros((runs, len(N_list)))\n",
    "        for i in range(runs):\n",
    "            x, y = generate_data(N_max, batch_size, input_dim, objective, narrow)\n",
    "            model_copy = copy.deepcopy(model)\n",
    "            model_copy.reinit()\n",
    "            train(model_copy, x, y, iterations, lamb)\n",
    "            errors = generalization_error(N_list, 1000, model_copy, objective, narrow)\n",
    "            run_errors[i] = np.array(errors)\n",
    "        \n",
    "        mean_error = np.mean(run_errors, axis = 0)\n",
    "        std_error = np.std(run_errors, axis = 0)\n",
    "        if log_plot:\n",
    "            plt.semilogy(N_list, mean_error, label = model.__name__)\n",
    "        else:\n",
    "            plt.plot(N_list, mean_error, label = model.__name__)\n",
    "        plt.fill_between(N_list, mean_error - std_error, mean_error + std_error, alpha = 0.2)\n",
    "\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.ylim([1e-5, 1e-1]) \n",
    "    plt.xlabel(\"N\")\n",
    "    plt.ylabel(\"Mean Square Error\")\n",
    "    narrow_str = \"Narrow\" if narrow else \"Wide\"\n",
    "    plt.title(narrow_str + \" generalization for \" + objective.__name__)\n",
    "    scale_str = \"\" if not scaleup else \"scaled\"\n",
    "    plt.savefig(\"plots_high_dim/\" + objective.__name__ + \"_\" + narrow_str + \"_\" + str(input_dim) + scale_str)\n",
    "#     plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run to generate plots in Figure 1:\n",
    "\n",
    "N_max = 4\n",
    "hidden_dim = 50\n",
    "\n",
    "iterations = 1000\n",
    "batch_size = 100\n",
    "\n",
    "input_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently neuron\n",
      "currently neuron\n"
     ]
    }
   ],
   "source": [
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, mean, narrow = False, log_plot = True)\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim , mean, narrow = True, log_plot = True)\n",
    "\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, median, narrow = False, log_plot = True)\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim , median, narrow = True, log_plot = True)\n",
    "\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, maximum, narrow = False, log_plot = True)\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim , maximum, narrow = True, log_plot = True)\n",
    "\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, softmax, narrow = False, log_plot = True)\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim , softmax, narrow = True, log_plot = True)\n",
    "\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, second, narrow = False, log_plot = True)\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim , second, narrow = True, log_plot = True)\n",
    "\n",
    "compare_models(N_max, hidden_dim, iterations, batch_size, input_dim, neuron, narrow = False, log_plot = True)\n",
    "compare_models(N_max, hidden_dim, iterations, batch_size, input_dim , neuron, narrow = True, log_plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots for right half of Figure 2\n",
    "\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim , smooth_neuron, narrow = True, log_plot = True, scaleup = True)\n",
    "# compare_models(N_max, hidden_dim, iterations, batch_size, input_dim , neuron, narrow = True, log_plot = True, scaleup = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[108.8774642944336, 0.12319805473089218, 0.06409303098917007, 0.031051063910126686, 0.00372758018784225, 0.012634518556296825, 0.011655419133603573, 0.0014871725579723716, 0.010766642168164253, 0.0005567565094679594]\n",
      "f1 [0.88040781]\n",
      "[68.0797348022461, 0.2454993724822998, 0.14803007245063782, 0.03867511451244354, 0.03269072622060776, 0.00997860822826624, 0.02678658626973629, 0.014266696758568287, 0.021281693130731583, 0.010137232020497322]\n",
      "f2 [1.40248609]\n",
      "[109.46990966796875, 23.109466552734375, 12.243729591369629, 7.640867710113525, 5.95836067199707, 3.8359882831573486, 5.479048728942871, 2.708665370941162, 2.0167436599731445, 1.3769158124923706]\n",
      "f3 [10.7036314]\n",
      "[183.14517211914062, 0.0831356942653656, 0.04076262190937996, 0.012397709302604198, 0.004988738335669041, 0.0012710250448435545, 0.0005181847373023629, 0.0004428067186381668, 0.0003207512490916997, 2.5524159354972653e-06]\n",
      "f1 [0.73567802]\n",
      "[173.53285217285156, 0.39136749505996704, 0.21716876327991486, 0.3039013743400574, 0.10364838689565659, 0.1115407943725586, 0.04884850233793259, 0.03127552568912506, 0.050584495067596436, 0.012652428820729256]\n",
      "f2 [1.09764647]\n",
      "[171.61618041992188, 21.432239532470703, 9.250426292419434, 5.081326484680176, 8.578685760498047, 3.3549606800079346, 1.9707400798797607, 2.0513293743133545, 2.093308925628662, 1.786596417427063]\n",
      "f3 [8.66771984]\n",
      "[108.632080078125, 0.06711657345294952, 0.09157787263393402, 0.027800465002655983, 0.013339114375412464, 0.016260523349046707, 0.026769891381263733, 0.00915683712810278, 0.0007586986175738275, 0.0010069425916299224]\n",
      "f1 [0.94692403]\n",
      "[67.31257629394531, 0.5295701622962952, 0.10911281406879425, 0.09186068922281265, 0.021537896245718002, 0.05257705971598625, 0.016890516504645348, 0.03472427278757095, 0.005836267955601215, 0.016236500814557076]\n",
      "f2 [1.27436852]\n",
      "[182.3834228515625, 15.848860740661621, 10.520750045776367, 13.015400886535645, 5.76570463180542, 7.494740962982178, 6.042023658752441, 2.2020249366760254, 2.895684003829956, 1.71371328830719]\n",
      "f3 [10.43487167]\n",
      "[170.8079376220703, 0.1257227063179016, 0.042895425111055374, 0.08551076799631119, 0.004183998331427574, 0.004706754349172115, 0.032425813376903534, 0.0026444937102496624, 0.008510349318385124, 0.007632500026375055]\n",
      "f1 [1.00701511]\n",
      "[109.45751953125, 0.3265345096588135, 0.06437559425830841, 0.0829334482550621, 0.017944756895303726, 0.05911250039935112, 0.012444421648979187, 0.0022867671214044094, 0.002037152647972107, 0.005318568553775549]\n",
      "f2 [1.37603569]\n",
      "[68.02525329589844, 17.263832092285156, 7.94754695892334, 11.304899215698242, 5.856715679168701, 4.961756229400635, 4.057950496673584, 2.9562020301818848, 1.492393136024475, 1.3526421785354614]\n",
      "f3 [10.43960476]\n",
      "[92.71025085449219, 0.22982566058635712, 0.03871587663888931, 0.01773875392973423, 0.0067434050142765045, 0.030540015548467636, 0.002045274246484041, 0.008772049099206924, 0.004660188220441341, 0.0003214592288713902]\n",
      "f1 [0.99074161]\n",
      "[173.41697692871094, 0.331628680229187, 0.045253295451402664, 0.0461781844496727, 0.02402668446302414, 0.019154038280248642, 0.011913775466382504, 0.00505355279892683, 0.003111822996288538, 0.004491136875003576]\n",
      "f2 [1.17974555]\n",
      "[105.92518615722656, 20.676000595092773, 8.991873741149902, 10.699506759643555, 8.99560546875, 4.948197841644287, 5.360770225524902, 3.3611435890197754, 3.23832631111145, 1.6612589359283447]\n",
      "f3 [9.33539104]\n",
      "[67.9161376953125, 0.3690698444843292, 0.018646275624632835, 0.010265241377055645, 0.010564301162958145, 0.004501174204051495, 0.006261263974010944, 0.0003777384990826249, 0.00044782442273572087, 0.001002477714791894]\n",
      "f1 [0.94678974]\n",
      "[110.2784652709961, 0.10071267932653427, 0.11152008920907974, 0.044773176312446594, 0.024464525282382965, 0.017309818416833878, 0.010623741894960403, 0.013675791211426258, 0.019295087084174156, 0.0020108246244490147]\n",
      "f2 [1.19421124]\n",
      "[92.79186248779297, 40.39788055419922, 7.662722110748291, 11.75561809539795, 4.5903778076171875, 6.816597938537598, 3.3188469409942627, 2.0440194606781006, 2.0320169925689697, 3.2058777809143066]\n",
      "f3 [9.34089279]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-576c883b63d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSymmetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-bc64c6fe1869>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, x, y, iterations, lamb)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlamb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/prime/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/prime/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_max = 4\n",
    "hidden_dim = 50\n",
    "iterations = 1000\n",
    "batch_size = 100\n",
    "\n",
    "objective = neuron\n",
    "narrow = False\n",
    "\n",
    "input_dim = 10\n",
    "\n",
    "\n",
    "x, y = generate_data(N_max, batch_size, input_dim, objective, narrow)\n",
    "\n",
    "for i in range(20):\n",
    "    model = Symmetric(input_dim, 2000, hidden_dim)\n",
    "    model.train()\n",
    "    losses = train(model, x, y, iterations, lamb = 0.000)\n",
    "    model.eval()\n",
    "    print(losses[::int(iterations/10)])\n",
    "    print(\"f1\", generalization_error([4], 5000, model, objective, narrow))\n",
    "    \n",
    "    model = KNN(input_dim, 2000, hidden_dim)\n",
    "    model.train()\n",
    "    losses = train(model, x, y, iterations, lamb = 0.000)\n",
    "    model.eval()\n",
    "    print(losses[::int(iterations/10)])\n",
    "    print(\"f2\", generalization_error([4], 5000, model, objective, narrow))\n",
    "    \n",
    "    model = KK(input_dim, 2000, 1000)\n",
    "    model.train()\n",
    "    losses = train(model, x, y, iterations, lamb = 0.0001)\n",
    "    model.eval()\n",
    "    print(losses[::int(iterations/10)])\n",
    "    print(\"f3\", generalization_error([4], 5000, model, objective, narrow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for Figure 3\n",
    "\n",
    "# N_max = 4\n",
    "# hidden_dim = 50\n",
    "\n",
    "# iterations = 1000\n",
    "# batch_size = 100\n",
    "\n",
    "# input_dim = 5\n",
    "\n",
    "# objective = mean\n",
    "# narrow = False\n",
    "\n",
    "# log_plot = True\n",
    "\n",
    "# f1 = Symmetric(input_dim, 1000, hidden_dim)\n",
    "# f2 = DeepSets(input_dim, 1000, hidden_dim)\n",
    "# f1.__name__ = \"S1\"\n",
    "# f2.__name__ = \"DeepSets\"\n",
    "\n",
    "# models = [f1, f2]\n",
    "\n",
    "# N_list = np.arange(2, N_max + 16)\n",
    "\n",
    "# for model in models:\n",
    "#     x, y = generate_data(N_max, batch_size, input_dim, objective, narrow)\n",
    "\n",
    "#     lamb = 0.\n",
    "\n",
    "#     runs = 10\n",
    "#     run_errors = np.zeros((runs, len(N_list)))\n",
    "#     for i in range(runs):\n",
    "#         x, y = generate_data(N_max, batch_size, input_dim, objective, narrow)\n",
    "#         model_copy = copy.deepcopy(model)\n",
    "#         model_copy.reinit()\n",
    "#         train(model_copy, x, y, iterations, lamb)\n",
    "#         errors = generalization_error(N_list, 1000, model_copy, objective, narrow)\n",
    "#         run_errors[i] = np.array(errors)\n",
    "\n",
    "#     mean_error = np.mean(run_errors, axis = 0)\n",
    "#     std_error = np.std(run_errors, axis = 0)\n",
    "#     if log_plot:\n",
    "#         plt.semilogy(N_list, mean_error, label = model.__name__)\n",
    "#     else:\n",
    "#         plt.plot(N_list, mean_error, label = model.__name__)\n",
    "#     plt.fill_between(N_list, mean_error - std_error, mean_error + std_error, alpha = 0.2)\n",
    "\n",
    "\n",
    "# plt.legend()\n",
    "# #     plt.ylim([1e-5, 1e10]) \n",
    "# plt.xlabel(\"N\")\n",
    "# plt.ylabel(\"Mean Square Error\")\n",
    "# narrow_str = \"Narrow\" if narrow else \"Wide\"\n",
    "# plt.title(\"Normalized vs. Unnormalized generalization for \" + objective.__name__)\n",
    "# plt.savefig(\"plots_high_dim/\" + \"deepsets\")\n",
    "# #     plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
